{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c5f846e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Setup complete. Edit backend: editdistance | wordfreq: True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ================================================\n",
    "# SECTION 1 — SETUP & UTILS\n",
    "# ================================================\n",
    "import sys, os, random, math, json, string, warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def _ensure(pkgs):\n",
    "    \"\"\"Best-effort installs. Notebook still runs if offline; LLM tries fallbacks.\"\"\"\n",
    "    import importlib, subprocess, sys as _sys\n",
    "    for p in pkgs:\n",
    "        base = p.split(\">=\")[0]\n",
    "        try:\n",
    "            importlib.import_module(base)\n",
    "        except Exception:\n",
    "            try:\n",
    "                print(f\"Installing {p} …\")\n",
    "                subprocess.check_call([_sys.executable, \"-m\", \"pip\", \"install\", p])\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ Could not install {p}: {e}\")\n",
    "\n",
    "_ensure([\"numpy\", \"pandas\", \"matplotlib\", \"wordfreq\", \"editdistance\",\n",
    "         \"transformers>=4.41.0\", \"sentencepiece>=0.1.99\", \"torch\", \"ipywidgets\", \"datasets\"])\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Levenshtein backend\n",
    "try:\n",
    "    import editdistance as _editdistance\n",
    "    _USE_EDITDIST = \"editdistance\"\n",
    "except Exception:\n",
    "    _editdistance = None\n",
    "    try:\n",
    "        from rapidfuzz.distance import Levenshtein\n",
    "        _USE_EDITDIST = \"rapidfuzz\"\n",
    "    except Exception:\n",
    "        Levenshtein = None\n",
    "        _USE_EDITDIST = \"none\"\n",
    "\n",
    "# Wordfreq\n",
    "try:\n",
    "    from wordfreq import top_n_list, word_frequency, zipf_frequency\n",
    "    _HAS_WORDFREQ = True\n",
    "except Exception:\n",
    "    _HAS_WORDFREQ = False\n",
    "\n",
    "# LLM libs\n",
    "from transformers import AutoTokenizer, AutoConfig\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoModelForCausalLM\n",
    "\n",
    "# Repro\n",
    "random.seed(13)\n",
    "np.random.seed(13)\n",
    "\n",
    "print(\"✅ Setup complete. Edit backend:\", _USE_EDITDIST, \"| wordfreq:\", _HAS_WORDFREQ)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3592bc07",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ================================================\n",
    "# SECTION 2 — DATA & NOISE\n",
    "# ================================================\n",
    "from collections import defaultdict, Counter\n",
    "from typing import List, Tuple, Dict, Iterable, Set\n",
    "ALPHABET = string.ascii_lowercase\n",
    "\n",
    "KEY_NEIGHBORS: Dict[str, str] = {\n",
    "    'q':'was', 'w':'qesd', 'e':'wsdr', 'r':'edft', 't':'rfgy', 'y':'tghu', 'u':'yhji', 'i':'ujko', 'o':'iklp', 'p':'ol',\n",
    "    'a':'qwsz', 's':'weadxz', 'd':'ersfcx', 'f':'rtdgcv', 'g':'tyfhbv', 'h':'yugjbn', 'j':'uikhmn', 'k':'ijolm', 'l':'opk',\n",
    "    'z':'asx', 'x':'zsdc', 'c':'xdfv', 'v':'cfgb', 'b':'vghn', 'n':'bhjm', 'm':'njk'\n",
    "}\n",
    "\n",
    "def random_neighbor(c: str) -> str:\n",
    "    c = c.lower()\n",
    "    neigh = KEY_NEIGHBORS.get(c, '')\n",
    "    return random.choice(neigh) if neigh else random.choice(ALPHABET)\n",
    "\n",
    "def noisy_word(word: str, p_del=0.05, p_ins=0.05, p_sub=0.06, p_trans=0.02, max_ops=2):\n",
    "    ops = 0; s = list(word); counts = defaultdict(int)\n",
    "    while ops < max_ops and s:\n",
    "        r = random.random()\n",
    "        if r < p_del and len(s)>1:\n",
    "            i = random.randrange(len(s)); s.pop(i); counts['del']+=1\n",
    "        elif r < p_del + p_ins:\n",
    "            i = random.randrange(len(s)+1)\n",
    "            s.insert(i, random_neighbor(s[i-1] if i>0 else random.choice(ALPHABET))); counts['ins']+=1\n",
    "        elif r < p_del + p_ins + p_sub:\n",
    "            i = random.randrange(len(s)); s[i] = random_neighbor(s[i]); counts['sub']+=1\n",
    "        elif r < p_del+p_ins+p_sub+p_trans and len(s)>1:\n",
    "            i = random.randrange(len(s)-1); s[i], s[i+1] = s[i+1], s[i]; counts['trans']+=1\n",
    "        else:\n",
    "            break\n",
    "        ops += 1\n",
    "    noisy = ''.join(s) or word\n",
    "    return noisy, counts\n",
    "\n",
    "def edits1(word: str, alphabet: str = ALPHABET) -> Set[str]:\n",
    "    w = word.lower()\n",
    "    splits = [(w[:i], w[i:]) for i in range(len(w)+1)]\n",
    "    deletes = [L + R[1:] for L, R in splits if R]\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "    replaces = [L + c + R[1:] for L, R in splits if R for c in alphabet]\n",
    "    inserts = [L + c + R for L, R in splits for c in alphabet]\n",
    "    return set(deletes+transposes+replaces+inserts)\n",
    "\n",
    "def edits2(word: str) -> Set[str]:\n",
    "    return set(e2 for e1 in edits1(word) for e2 in edits1(e1))\n",
    "\n",
    "def build_vocab(n=80000, min_len=3, max_len=18) -> List[str]:\n",
    "    if _HAS_WORDFREQ:\n",
    "        vocab = [w for w in top_n_list(\"en\", n) if w.isalpha() and min_len <= len(w) <= max_len]\n",
    "        seen=set(); out=[]\n",
    "        for w in vocab:\n",
    "            wl = w.lower()\n",
    "            if wl not in seen:\n",
    "                seen.add(wl); out.append(wl)\n",
    "        return out\n",
    "    # Minimal fallback\n",
    "    return [\"the\",\"and\",\"there\",\"their\",\"cat\",\"dog\",\"apple\",\"banana\",\"orange\",\"grape\",\"house\",\n",
    "            \"keyboard\",\"screen\",\"computer\",\"program\",\"python\",\"model\",\"language\",\"learning\",\n",
    "            \"school\",\"teacher\",\"student\",\"book\",\"library\",\"garden\",\"flower\",\"river\",\"mountain\",\n",
    "            \"beautiful\",\"strength\",\"tomorrow\",\"yesterday\",\"because\",\"receive\",\"believe\",\"friend\"]\n",
    "\n",
    "def synth_pairs(words: List[str], per_word: int = 3) -> pd.DataFrame:\n",
    "    rows=[]\n",
    "    for w in words:\n",
    "        for _ in range(per_word):\n",
    "            noisy, types = noisy_word(w)\n",
    "            rows.append({\"noisy\": noisy, \"clean\": w, \"types\": dict(types)})\n",
    "    return pd.DataFrame(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "737682ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ data/splits written\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>noisy</th>\n",
       "      <th>clean</th>\n",
       "      <th>types</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>wonderful</td>\n",
       "      <td>wonderful</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>wonderful</td>\n",
       "      <td>wonderful</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>wonderful</td>\n",
       "      <td>wonderful</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>conditioning</td>\n",
       "      <td>conditioning</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>conditioning</td>\n",
       "      <td>conditioning</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          noisy         clean types\n",
       "0     wonderful     wonderful    {}\n",
       "1     wonderful     wonderful    {}\n",
       "2     wonderful     wonderful    {}\n",
       "3  conditioning  conditioning    {}\n",
       "4  conditioning  conditioning    {}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# ================================================\n",
    "# SECTION 3 — DATA SPLITS\n",
    "# ================================================\n",
    "vocab = build_vocab(80000)\n",
    "random.shuffle(vocab)\n",
    "n = len(vocab)\n",
    "train_words = vocab[: int(n*0.8)]\n",
    "dev_words   = vocab[int(n*0.8): int(n*0.9)]\n",
    "test_words  = vocab[int(n*0.9):]\n",
    "\n",
    "train_df = synth_pairs(train_words, 3)\n",
    "dev_df   = synth_pairs(dev_words,   3)\n",
    "test_df  = synth_pairs(test_words,  3)\n",
    "\n",
    "os.makedirs(\"data/splits\", exist_ok=True)\n",
    "train_df.to_json(\"data/splits/train.jsonl\", lines=True, orient=\"records\")\n",
    "dev_df.to_json(  \"data/splits/dev.jsonl\",   lines=True, orient=\"records\")\n",
    "test_df.to_json( \"data/splits/test.jsonl\",  lines=True, orient=\"records\")\n",
    "\n",
    "print(\"✅ data/splits written\"); train_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dde7fffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ================================================\n",
    "# SECTION 4 — N-GRAM LM (CHAR)\n",
    "# ================================================\n",
    "BOS=\"^\"; EOS=\"$\"\n",
    "class CharNGramLM:\n",
    "    def __init__(self, n=3, alpha=0.5):\n",
    "        self.n=n; self.alpha=alpha\n",
    "        self.context_counts=defaultdict(Counter)\n",
    "        self.context_totals=Counter()\n",
    "        self.vocab_chars=set()\n",
    "    def fit(self, words: Iterable[str]):\n",
    "        for w in words:\n",
    "            s=(BOS*(self.n-1))+w+EOS\n",
    "            self.vocab_chars.update(s)\n",
    "            for i in range(self.n-1, len(s)):\n",
    "                ctx=s[i-(self.n-1):i]; nxt=s[i]\n",
    "                self.context_counts[ctx][nxt]+=1\n",
    "                self.context_totals[ctx]+=1\n",
    "        return self\n",
    "    def log_prob(self, word: str)->float:\n",
    "        s=(BOS*(self.n-1))+word+EOS\n",
    "        V=max(1,len(self.vocab_chars)); lp=0.0\n",
    "        for i in range(self.n-1, len(s)):\n",
    "            ctx=s[i-(self.n-1):i]; nxt=s[i]\n",
    "            c=self.context_counts[ctx][nxt]; tot=self.context_totals[ctx]\n",
    "            p=(c+self.alpha)/(tot+self.alpha*V)\n",
    "            lp+=math.log(p)\n",
    "        return lp\n",
    "\n",
    "def log_unigram_prior(word: str, lang=\"en\")->float:\n",
    "    if _HAS_WORDFREQ:\n",
    "        f=max(word_frequency(word, lang, minimum=1e-9), 1e-12)\n",
    "        return math.log(f)\n",
    "    return -0.001*len(word)\n",
    "\n",
    "def build_vocab_set(n=100_000)->Set[str]:\n",
    "    return set(map(str.lower, build_vocab(n)))\n",
    "\n",
    "def candidates(word: str, vocab: Set[str]):\n",
    "    w=word.lower(); cands=set()\n",
    "    if w in vocab: cands.add(w)\n",
    "    cands |= (edits1(w) & vocab)\n",
    "    if not cands: cands |= (edits2(w) & vocab)\n",
    "    return list(cands) or [w]\n",
    "\n",
    "def score(word: str, lm: CharNGramLM, lambda_prior=0.4):\n",
    "    return lm.log_prob(word) + lambda_prior*log_unigram_prior(word)\n",
    "\n",
    "def cer(ref: str, hyp: str)->float:\n",
    "    if _editdistance is not None:\n",
    "        return _editdistance.eval(ref, hyp)/max(1,len(ref))\n",
    "    if 'Levenshtein' in globals() and Levenshtein is not None:\n",
    "        return Levenshtein.distance(ref, hyp)/max(1,len(ref))\n",
    "    return float(ref!=hyp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7fea8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ================================================\n",
    "# SECTION 5 — TRAIN & EVAL N-GRAM\n",
    "# ================================================\n",
    "vocab_set = build_vocab_set(100_000)\n",
    "lm = CharNGramLM(n=3, alpha=0.5).fit(vocab_set)\n",
    "\n",
    "def eval_split(df, lm, vocab, topk=(1,5)):\n",
    "    correct={k:0 for k in topk}; tot=0; cer_sum=0.0\n",
    "    for _, row in df.iterrows():\n",
    "        noisy, gold = row[\"noisy\"], row[\"clean\"]\n",
    "        ranked = sorted(candidates(noisy, vocab), key=lambda w: score(w, lm), reverse=True)\n",
    "        tot += 1\n",
    "        for k in topk:\n",
    "            if gold in set(ranked[:k]): correct[k]+=1\n",
    "        cer_sum += cer(gold, ranked[0])\n",
    "    out={f\"top{k}_acc\": correct[k]/tot for k in topk}\n",
    "    out.update({\"avg_cer\": cer_sum/tot, \"n\": tot})\n",
    "    return out\n",
    "\n",
    "dev_m = eval_split(dev_df, lm, vocab_set)\n",
    "test_m = eval_split(test_df, lm, vocab_set)\n",
    "\n",
    "os.makedirs(\"reports\", exist_ok=True)\n",
    "with open(\"reports/ngram_metrics.json\",\"w\") as f: json.dump({\"dev\":dev_m,\"test\":test_m}, f, indent=2)\n",
    "\n",
    "print(\"✅ n-gram metrics\")\n",
    "print(json.dumps({\"dev\":dev_m,\"test\":test_m}, indent=2))\n",
    "\n",
    "plt.figure()\n",
    "plt.bar([\"Top-1\",\"Top-5\"], [test_m.get(\"top1_acc\",0.0), test_m.get(\"top5_acc\",0.0)])\n",
    "plt.ylim(0,1); plt.title(\"N-gram accuracy (test)\"); plt.tight_layout()\n",
    "plt.savefig(\"reports/ngram_plots.png\"); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17dde7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ================================================\n",
    "# SECTION 5.1 — SENTENCE-LEVEL CORRECTION (N-GRAM)\n",
    "# ================================================\n",
    "import re\n",
    "TOKEN_PATTERN = re.compile(r\"\\w+|[^\\w\\s]\", re.UNICODE)\n",
    "\n",
    "def _match_case(src: str, tgt: str)->str:\n",
    "    if src.isupper(): return tgt.upper()\n",
    "    if src[:1].isupper(): return tgt[:1].upper()+tgt[1:]\n",
    "    return tgt\n",
    "\n",
    "def correct_sentence_ngram(text: str, lm: CharNGramLM, vocab: Set[str], topk=5):\n",
    "    # Simple & stable: per-token correction + punctuation-aware spacing\n",
    "    toks = TOKEN_PATTERN.findall(text)\n",
    "    out=[]\n",
    "    for t in toks:\n",
    "        if t.isalpha():\n",
    "            ranked = sorted(candidates(t, vocab), key=lambda w: score(w, lm), reverse=True)\n",
    "            best = ranked[0] if ranked else t\n",
    "            out.append(_match_case(t, best))\n",
    "        else:\n",
    "            out.append(t)\n",
    "    # detokenize: no space before .,!?:;) and after opening brackets\n",
    "    detok=[]\n",
    "    for i, t in enumerate(out):\n",
    "        if i>0 and re.match(r\"[.,!?;:)\\]\\}]\", t):\n",
    "            detok[-1] = detok[-1]+t\n",
    "        elif t in [\"(\", \"[\", \"{\"]:\n",
    "            detok.append(t)\n",
    "        else:\n",
    "            detok.append((t if not detok else \" \"+t))\n",
    "    return \"\".join(detok).lstrip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762460e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ================================================\n",
    "# SECTION 6 — OPTIONAL QUICK T5 FINE-TUNE\n",
    "# ================================================\n",
    "# You can skip this entirely. LLM fallback below works without training.\n",
    "def t5_train_and_eval(train_df, dev_df, test_df, out_dir=\"models/t5_speller\",\n",
    "                      max_train=8000, max_dev=2000, epochs=1, batch_size=32):\n",
    "    try:\n",
    "        from transformers import (T5ForConditionalGeneration, T5TokenizerFast,\n",
    "                                  DataCollatorForSeq2Seq, Trainer, TrainingArguments)\n",
    "        from datasets import Dataset\n",
    "        import numpy as np\n",
    "        os.makedirs(out_dir, exist_ok=True)\n",
    "        MAX_LEN=32; MODEL_NAME=\"t5-small\"\n",
    "\n",
    "        def make_hf(df):\n",
    "            return Dataset.from_pandas(df[[\"noisy\",\"clean\"]])\n",
    "        def preprocess(tok, ex):\n",
    "            inputs=[f\"spell: {x}\" for x in ex[\"noisy\"]]; targets=ex[\"clean\"]\n",
    "            mi=tok(inputs, max_length=MAX_LEN, truncation=True)\n",
    "            with tok.as_target_tokenizer():\n",
    "                labs=tok(targets, max_length=MAX_LEN, truncation=True)\n",
    "            mi[\"labels\"]=labs[\"input_ids\"]; return mi\n",
    "        def exact_match(ps, gs): return sum(p==g for p,g in zip(ps,gs))/max(1,len(gs))\n",
    "\n",
    "        from transformers import T5TokenizerFast, T5ForConditionalGeneration\n",
    "        tok=T5TokenizerFast.from_pretrained(MODEL_NAME)\n",
    "        model=T5ForConditionalGeneration.from_pretrained(MODEL_NAME)\n",
    "\n",
    "        train_df=train_df.sample(n=min(max_train,len(train_df)), random_state=13)\n",
    "        dev_df  =dev_df  .sample(n=min(max_dev,  len(dev_df)),   random_state=13)\n",
    "\n",
    "        from datasets import Dataset\n",
    "        tr=make_hf(train_df); dv=make_hf(dev_df)\n",
    "        tr_tok=tr.map(lambda e: preprocess(tok,e), batched=True, remove_columns=tr.column_names)\n",
    "        dv_tok=dv.map(lambda e: preprocess(tok,e), batched=True, remove_columns=dv.column_names)\n",
    "\n",
    "        collator=DataCollatorForSeq2Seq(tok, model=model)\n",
    "        args = TrainingArguments(output_dir=out_dir, evaluation_strategy=\"epoch\", save_strategy=\"epoch\",\n",
    "                                 logging_steps=100, per_device_train_batch_size=batch_size,\n",
    "                                 per_device_eval_batch_size=batch_size, num_train_epochs=epochs,\n",
    "                                 learning_rate=5e-4, weight_decay=0.0, predict_with_generate=True, fp16=False)\n",
    "        def cer_metric(refs, hyps): return sum(cer(r,h) for r,h in zip(refs,hyps))/max(1,len(refs))\n",
    "        def compute_metrics(eval_pred):\n",
    "            p_ids, y_ids = eval_pred\n",
    "            p_txt=tok.batch_decode(p_ids, skip_special_tokens=True)\n",
    "            y_ids[y_ids==-100]=tok.pad_token_id\n",
    "            g_txt=tok.batch_decode(y_ids, skip_special_tokens=True)\n",
    "            return {\"exact_match\": exact_match(p_txt,g_txt), \"cer\": cer_metric(g_txt,p_txt)}\n",
    "\n",
    "        from transformers import Trainer\n",
    "        trnr=Trainer(model=model, args=args, train_dataset=tr_tok, eval_dataset=dv_tok,\n",
    "                     data_collator=collator, tokenizer=tok, compute_metrics=compute_metrics)\n",
    "        trnr.train()\n",
    "        trnr.save_model(out_dir); tok.save_pretrained(out_dir)\n",
    "        print(\"✅ Saved fine-tuned model to\", out_dir)\n",
    "    except Exception as e:\n",
    "        print(\"⚠️ Skipping fine-tune:\", e)\n",
    "\n",
    "# Example (optional):\n",
    "# t5_train_and_eval(train_df, dev_df, test_df, max_train=8000, max_dev=2000, epochs=1, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad045f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local T5 not found: We couldn't connect to 'https://huggingface.co' to load the files, and couldn't find them in the cached files.\n",
      "Check your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e27a60d6c2b6405f9476799e41edb797",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/353 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7aaad8f5a6c48bcb0fba75526f97418",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ef7e3d3073448ab8c92d3ff39dfe193",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6882df797884578b10dc541064851e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1d305c3d8e94cf2b72c90dff8da2957",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bad6efcf0d5e4711b746b11b46f5456d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "299db1ab89854957b3413b5b95aa1b0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)f3815f99f42fcc48674a6f5d4e785cbc16448b80:   0%|          | 0.00/558M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded LLM: oliverguhr/spelling-correction-english-base | seq2seq=True | device=cpu\n"
     ]
    }
   ],
   "source": [
    "# ================================================\n",
    "# SECTION 6.1 — ROBUST LLM LOADER (NO TRAINING NEEDED, STRONG PROMPTS)\n",
    "# ================================================\n",
    "import os, torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForCausalLM\n",
    "\n",
    "# Use a writable HF cache\n",
    "os.environ.setdefault(\"HF_HOME\", \"./.hf_cache\")\n",
    "os.makedirs(os.environ[\"HF_HOME\"], exist_ok=True)\n",
    "\n",
    "DEVICE = torch.device(\"cpu\")  # CPU-only for reliability\n",
    "\n",
    "def _try_load_seq2seq(name, local=False):\n",
    "    tok = AutoTokenizer.from_pretrained(name, local_files_only=local)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(name, local_files_only=local)\n",
    "    model.to(DEVICE).eval()\n",
    "    return tok, model, name, True  # seq2seq=True\n",
    "\n",
    "def _try_load_causal(name, local=False):\n",
    "    tok = AutoTokenizer.from_pretrained(name, local_files_only=local)\n",
    "    # GPT-2 usually lacks pad_token; set to EOS to avoid warnings\n",
    "    if tok.pad_token is None and tok.eos_token is not None:\n",
    "        tok.pad_token = tok.eos_token\n",
    "    model = AutoModelForCausalLM.from_pretrained(name, local_files_only=local)\n",
    "    model.to(DEVICE).eval()\n",
    "    return tok, model, name, False  # seq2seq=False\n",
    "\n",
    "def load_llm_with_fallbacks():\n",
    "    # 1) your fine-tuned T5 (if you trained later)\n",
    "    try:\n",
    "        return _try_load_seq2seq(\"models/t5_speller\", local=True)\n",
    "    except Exception as e:\n",
    "        print(\"Local T5 not found:\", e)\n",
    "\n",
    "    # 2) specialized correction models (better than generic small models)\n",
    "    for m in [\n",
    "        \"oliverguhr/spelling-correction-english-base\",  # spelling-focused T5\n",
    "        \"vennify/t5-base-grammar-correction\",          # grammar correction T5\n",
    "    ]:\n",
    "        try:\n",
    "            return _try_load_seq2seq(m, local=False)\n",
    "        except Exception as e:\n",
    "            print(f\"{m} not available:\", e)\n",
    "\n",
    "    # 3) FLAN-T5 (instruction-tuned)\n",
    "    for m in [\"google/flan-t5-base\", \"google/flan-t5-small\"]:\n",
    "        try:\n",
    "            return _try_load_seq2seq(m, local=False)\n",
    "        except Exception as e:\n",
    "            print(f\"{m} not available:\", e)\n",
    "\n",
    "    # 4) GPT-2 (causal LM) as a last resort before vanilla T5\n",
    "    try:\n",
    "        return _try_load_causal(\"gpt2\", local=False)\n",
    "    except Exception as e:\n",
    "        print(\"gpt2 not available:\", e)\n",
    "\n",
    "    # 5) vanilla T5-small\n",
    "    return _try_load_seq2seq(\"t5-small\", local=False)\n",
    "\n",
    "tok_llm, llm_model, llm_name, IS_SEQ2SEQ = load_llm_with_fallbacks()\n",
    "print(f\"✅ Loaded LLM: {llm_name} | seq2seq={IS_SEQ2SEQ} | device={DEVICE}\")\n",
    "\n",
    "# ---------- Helpers ----------\n",
    "def _generate_seq2seq(prompt: str) -> str:\n",
    "    ids = tok_llm([prompt], return_tensors=\"pt\")\n",
    "    ids = {k: v.to(DEVICE) for k, v in ids.items()}\n",
    "    with torch.inference_mode():\n",
    "        out = llm_model.generate(\n",
    "            **ids,\n",
    "            max_new_tokens=96,\n",
    "            num_beams=6,\n",
    "            length_penalty=1.0,\n",
    "            early_stopping=True,\n",
    "            do_sample=False,\n",
    "        )\n",
    "    return tok_llm.batch_decode(out, skip_special_tokens=True)[0].strip()\n",
    "\n",
    "def _generate_causal(prompt: str) -> str:\n",
    "    if tok_llm.pad_token is None and tok_llm.eos_token is not None:\n",
    "        tok_llm.pad_token = tok_llm.eos_token\n",
    "    ids = tok_llm(prompt, return_tensors=\"pt\")\n",
    "    ids = {k: v.to(DEVICE) for k, v in ids.items()}\n",
    "    with torch.inference_mode():\n",
    "        out = llm_model.generate(\n",
    "            **ids,\n",
    "            max_new_tokens=96,\n",
    "            num_beams=6,\n",
    "            early_stopping=True,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tok_llm.pad_token_id,\n",
    "        )\n",
    "    return tok_llm.decode(out[0], skip_special_tokens=True).strip()\n",
    "\n",
    "def _unchanged(a: str, b: str) -> bool:\n",
    "    return a.strip() == b.strip()\n",
    "\n",
    "# Few-shot examples help smaller models a lot\n",
    "FEWSHOT = (\n",
    "    \"Correct spelling and grammar.\\n\"\n",
    "    \"Input: Ths sentnce has soem mispelings.\\n\"\n",
    "    \"Corrected: This sentence has some misspellings.\\n\\n\"\n",
    "    \"Input: I cant beleive tehres no mlik.\\n\"\n",
    "    \"Corrected: I can't believe there's no milk.\\n\\n\"\n",
    ")\n",
    "\n",
    "def llm_correct_sentence(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Robust sentence correction:\n",
    "    - Tries multiple strong prompts in order, falls back if unchanged.\n",
    "    - Works for both seq2seq (T5/FLAN) and causal (GPT-2).\n",
    "    \"\"\"\n",
    "    text = (text or \"\").strip()\n",
    "    if not text:\n",
    "        return \"\"\n",
    "\n",
    "    # Strategy list varies slightly by model family, but we can try them in order.\n",
    "    if IS_SEQ2SEQ:\n",
    "        prompts = []\n",
    "\n",
    "        # Specialized models often expect plain or \"spell:\" input\n",
    "        if \"oliverguhr/spelling-correction\" in llm_name.lower():\n",
    "            prompts += [text, f\"spell: {text}\"]\n",
    "\n",
    "        if \"vennify/t5-base-grammar-correction\" in llm_name.lower():\n",
    "            prompts += [f\"fix grammar: {text}\", f\"proofread: {text}\"]\n",
    "\n",
    "        # FLAN prefers instruction format\n",
    "        if \"flan\" in llm_name.lower():\n",
    "            prompts += [\n",
    "                f\"Proofread and correct spelling and grammar.\\nReturn only the corrected sentence.\\nInput: {text}\\nCorrected:\",\n",
    "                FEWSHOT + f\"Input: {text}\\nCorrected:\",\n",
    "            ]\n",
    "\n",
    "        # Generic T5 fallbacks\n",
    "        prompts += [f\"fix spelling: {text}\", f\"spell: {text}\"]\n",
    "\n",
    "        # Try candidates until we see a change\n",
    "        for p in prompts:\n",
    "            out = _generate_seq2seq(p)\n",
    "            if not _unchanged(out, text):\n",
    "                return out\n",
    "        # If all unchanged, return the last attempt (or original)\n",
    "        return out if prompts else text\n",
    "\n",
    "    else:\n",
    "        # GPT-2: build an instruction + few-shot prompt and extract after \"Corrected:\"\n",
    "        prompt_variants = [\n",
    "            FEWSHOT + f\"Input: {text}\\nCorrected:\",\n",
    "            f\"Correct spelling and grammar.\\nInput: {text}\\nCorrected:\",\n",
    "        ]\n",
    "        for p in prompt_variants:\n",
    "            gen = _generate_causal(p)\n",
    "            # extract after \"Corrected:\"\n",
    "            if \"Corrected:\" in gen:\n",
    "                cand = gen.split(\"Corrected:\", 1)[1].strip()\n",
    "            else:\n",
    "                cand = gen\n",
    "            if not _unchanged(cand, text):\n",
    "                return cand\n",
    "        return cand  # last try\n",
    "\n",
    "# If your GUI calls `correct_with_t5`, point it here:\n",
    "try:\n",
    "    correct_with_t5 = llm_correct_sentence  # make GUI pick this up without changes\n",
    "except Exception:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ec715e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "126b81b70bdd42f183b8b8805e36487d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Textarea(value='Ths sentnce has soem mispelings.', description='Input:', layout=Layout(height='80px', width='1…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f26333e0aa3410fbe3f4429fefb449a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(button_style='primary', description='Correct', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "722ac7a8f3f4402f99aa591dad5cfd4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ GUI ready. Type text and click Correct.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ================================================\n",
    "# SECTION 7 — SIMPLE GUI (IPYWIDGETS): N-GRAM + LLM\n",
    "# ================================================\n",
    "try:\n",
    "    import ipywidgets as widgets\n",
    "    from IPython.display import display\n",
    "    ta = widgets.Textarea(\n",
    "        value=\"Ths sentnce has soem mispelings.\",\n",
    "        description=\"Input:\",\n",
    "        layout=widgets.Layout(width=\"100%\", height=\"80px\")\n",
    "    )\n",
    "    btn = widgets.Button(description=\"Correct\", button_style=\"primary\")\n",
    "    out = widgets.Output()\n",
    "\n",
    "    def on_click(_):\n",
    "        with out:\n",
    "            out.clear_output()\n",
    "            print(\"— N-gram sentence correction —\")\n",
    "            print(correct_sentence_ngram(ta.value, lm, vocab_set, topk=5))\n",
    "            print(\"\\n— LLM sentence correction —\")\n",
    "            print(llm_correct_sentence(ta.value))\n",
    "\n",
    "    btn.on_click(on_click)\n",
    "    display(ta, btn, out)\n",
    "    print(\"✅ GUI ready. Type text and click Correct.\")\n",
    "except Exception as e:\n",
    "    print(\"ipywidgets not available. You can call the functions directly:\")\n",
    "    print(\"correct_sentence_ngram('text', lm, vocab_set)  and  llm_correct_sentence('text')\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42be946b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
